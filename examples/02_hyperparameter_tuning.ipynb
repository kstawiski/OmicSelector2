{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OmicSelector2: Hyperparameter Tuning & Cross-Validation\n",
    "\n",
    "This notebook demonstrates advanced model training with hyperparameter optimization and cross-validation.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand cross-validation strategies\n",
    "- Optimize hyperparameters with Optuna\n",
    "- Use training callbacks (early stopping, model checkpointing)\n",
    "- Evaluate model stability across folds\n",
    "\n",
    "**Prerequisites:**\n",
    "```bash\n",
    "pip install omicselector2 optuna\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import OmicSelector2 components\n",
    "from omicselector2.models.classical import RandomForestClassifier as RFClassifier\n",
    "from omicselector2.models.classical import XGBoostClassifier\n",
    "from omicselector2.training.trainer import Trainer\n",
    "from omicselector2.training.callbacks import EarlyStopping, ModelCheckpoint, ProgressLogger\n",
    "from omicselector2.training.cross_validation import StratifiedKFoldSplitter, CrossValidator\n",
    "from omicselector2.training.evaluator import ClassificationEvaluator\n",
    "from omicselector2.training.hyperparameter import HyperparameterOptimizer, PREDEFINED_SEARCH_SPACES\n",
    "from omicselector2.features.classical.random_forest import RandomForestSelector\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure visualization\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic biomarker data\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=100,\n",
    "    n_informative=20,\n",
    "    n_redundant=10,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    flip_y=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "X = pd.DataFrame(X, columns=[f\"gene_{i}\" for i in range(X.shape[1])])\n",
    "y = pd.Series(y, name=\"response\")\n",
    "\n",
    "print(f\"Dataset: {X.shape}\")\n",
    "print(f\"Class distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection\n",
    "\n",
    "First, select the most important features to reduce dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 30 features using Random Forest\n",
    "selector = RandomForestSelector(\n",
    "    n_estimators=100,\n",
    "    n_features_to_select=30,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "print(f\"Selected features: {X_selected.shape[1]}\")\n",
    "print(f\"Top 10 features: {selector.selected_features_[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Validation Strategy\n",
    "\n",
    "Use stratified k-fold cross-validation to ensure balanced class distribution in each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-validation splitter\n",
    "cv_splitter = StratifiedKFoldSplitter(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Visualize fold distribution\n",
    "fold_info = []\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(cv_splitter.split(X_selected, y)):\n",
    "    fold_info.append({\n",
    "        'Fold': fold_idx + 1,\n",
    "        'Train Size': len(train_idx),\n",
    "        'Val Size': len(val_idx),\n",
    "        'Train Class 0': y.iloc[train_idx].value_counts()[0],\n",
    "        'Train Class 1': y.iloc[train_idx].value_counts()[1]\n",
    "    })\n",
    "\n",
    "fold_df = pd.DataFrame(fold_info)\n",
    "print(\"\\nCross-validation folds:\")\n",
    "print(fold_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Model with Callbacks\n",
    "\n",
    "Use callbacks to monitor training and save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for demonstration\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = RFClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=5,\n",
    "        min_delta=0.001,\n",
    "        mode='max'\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath='best_model.pkl',\n",
    "        monitor='val_auc',\n",
    "        save_best_only=True,\n",
    "        mode='max'\n",
    "    ),\n",
    "    ProgressLogger()\n",
    "]\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    callbacks=callbacks,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"Training with callbacks...\")\n",
    "history = trainer.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['auc'], label='Train AUC')\n",
    "plt.plot(history['val_auc'], label='Val AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('Model AUC')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Optimization with Optuna\n",
    "\n",
    "Use Bayesian optimization to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View predefined search spaces\n",
    "print(\"Predefined search spaces:\")\n",
    "for model_name, space in PREDEFINED_SEARCH_SPACES.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for param, values in space.items():\n",
    "        print(f\"  {param}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hyperparameter optimizer for Random Forest\n",
    "optimizer = HyperparameterOptimizer(\n",
    "    model_class=RFClassifier,\n",
    "    search_space='RandomForest',  # Use predefined space\n",
    "    metric='auc',\n",
    "    direction='maximize',\n",
    "    n_trials=20,  # Number of trials\n",
    "    cv=3,  # 3-fold CV\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "study = optimizer.optimize(X_selected, y, timeout=120)  # 2-minute timeout\n",
    "\n",
    "# Get best parameters\n",
    "best_params = optimizer.get_best_params()\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best CV AUC: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization history\n",
    "import optuna\n",
    "\n",
    "fig = optuna.visualization.plot_optimization_history(study)\n",
    "fig.show()\n",
    "\n",
    "# Plot parameter importances\n",
    "fig = optuna.visualization.plot_param_importances(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Best Model\n",
    "\n",
    "Train the final model with optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model (trained with best hyperparameters)\n",
    "best_model = optimizer.get_best_model(X_selected, y)\n",
    "\n",
    "print(\"Best model trained successfully!\")\n",
    "print(f\"Model parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation Evaluation\n",
    "\n",
    "Evaluate model stability across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-validator\n",
    "cv = CrossValidator(\n",
    "    model=RFClassifier(**best_params),\n",
    "    cv_strategy=cv_splitter,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "print(\"Performing 5-fold cross-validation...\")\n",
    "cv_results = cv.cross_validate(X_selected, y)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nCross-validation results:\")\n",
    "for fold_idx, metrics in enumerate(cv_results['fold_metrics']):\n",
    "    print(f\"\\nFold {fold_idx + 1}:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  AUC: {metrics['auc']:.4f}\")\n",
    "    print(f\"  F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "# Calculate mean and std\n",
    "mean_metrics = cv_results['mean_metrics']\n",
    "std_metrics = cv_results['std_metrics']\n",
    "\n",
    "print(\"\\nMean \u00b1 Std:\")\n",
    "print(f\"  Accuracy: {mean_metrics['accuracy']:.4f} \u00b1 {std_metrics['accuracy']:.4f}\")\n",
    "print(f\"  AUC: {mean_metrics['auc']:.4f} \u00b1 {std_metrics['auc']:.4f}\")\n",
    "print(f\"  F1: {mean_metrics['f1']:.4f} \u00b1 {std_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation results\n",
    "metrics_df = pd.DataFrame(cv_results['fold_metrics'])\n",
    "metrics_df['Fold'] = [f\"Fold {i+1}\" for i in range(len(metrics_df))]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, metric in enumerate(['accuracy', 'auc', 'f1']):\n",
    "    axes[idx].bar(metrics_df['Fold'], metrics_df[metric])\n",
    "    axes[idx].axhline(\n",
    "        y=mean_metrics[metric],\n",
    "        color='r',\n",
    "        linestyle='--',\n",
    "        label=f'Mean: {mean_metrics[metric]:.3f}'\n",
    "    )\n",
    "    axes[idx].set_ylabel(metric.upper())\n",
    "    axes[idx].set_title(f'{metric.upper()} per Fold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Cross-validation strategies** - Stratified k-fold for balanced evaluation\n",
    "2. **Training callbacks** - Early stopping and model checkpointing\n",
    "3. **Hyperparameter optimization** - Using Optuna for Bayesian optimization\n",
    "4. **Model stability evaluation** - Cross-validation metrics with mean \u00b1 std\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Always use cross-validation to assess model generalization\n",
    "- Hyperparameter tuning can significantly improve performance\n",
    "- Monitor multiple metrics (accuracy, AUC, F1) for comprehensive evaluation\n",
    "- Use callbacks to prevent overfitting and save best models\n",
    "\n",
    "**Next Steps:**\n",
    "- Learn about signature benchmarking (next notebook)\n",
    "- Try different models (XGBoost, Logistic Regression)\n",
    "- Experiment with custom search spaces\n",
    "- Apply to your own biomarker discovery problems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}