{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š Data Preprocessing & Quality Control with Google Drive\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kstawiski/OmicSelector2/blob/main/examples/04_data_preprocessing.ipynb)\n",
    "\n",
    "Learn how to prepare your own biomarker data for analysis with full Google Drive integration.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- ğŸ“ Load data from multiple formats (CSV, Excel, TSV) stored in Google Drive\n",
    "- ğŸ§¹ Quality control and data cleaning\n",
    "- ğŸ“ˆ Normalization methods (TPM, Log, Z-score)\n",
    "- ğŸ” Filter low-quality features\n",
    "- âœ‚ï¸ Train/test/validation splitting\n",
    "- âš–ï¸ Handle imbalanced datasets\n",
    "- ğŸ’¾ Save processed data back to Drive\n",
    "\n",
    "**Estimated Time**: 20-25 minutes  \n",
    "**Prerequisites**: Basic knowledge of Python and pandas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Setup: Install & Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OmicSelector2\n",
    "!pip install -q git+https://github.com/kstawiski/OmicSelector2.git\n",
    "!pip install -q openpyxl xlrd  # For Excel support\n",
    "\n",
    "print(\"âœ… Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "# Setup project directories\n",
    "BASE_DIR = '/content/drive/MyDrive/OmicSelector2'\n",
    "os.makedirs(f'{BASE_DIR}/data/raw', exist_ok=True)\n",
    "os.makedirs(f'{BASE_DIR}/data/processed', exist_ok=True)\n",
    "os.makedirs(f'{BASE_DIR}/data/qc_reports', exist_ok=True)\n",
    "os.makedirs(f'{BASE_DIR}/plots/preprocessing', exist_ok=True)\n",
    "\n",
    "print(f\"âœ… Drive mounted: {BASE_DIR}\")\n",
    "print(\"\\nğŸ“ Directory structure:\")\n",
    "print(f\"  â€¢ Raw data: {BASE_DIR}/data/raw/\")\n",
    "print(f\"  â€¢ Processed data: {BASE_DIR}/data/processed/\")\n",
    "print(f\"  â€¢ QC reports: {BASE_DIR}/data/qc_reports/\")\n",
    "print(f\"  â€¢ Plots: {BASE_DIR}/plots/preprocessing/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Step 1: Load Data from Google Drive\n",
    "\n",
    "We'll create sample data, but you can upload your own to `data/raw/` in Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo: Create sample data and save to Drive\n",
    "# (Replace this with your own data upload)\n",
    "\n",
    "print(\"ğŸ§¬ Creating sample gene expression dataset...\\n\")\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate raw counts (simulating RNA-seq)\n",
    "n_samples = 150\n",
    "n_genes = 1000\n",
    "\n",
    "# Simulate count data with some noise\n",
    "base_counts = np.random.negative_binomial(n=10, p=0.3, size=(n_samples, n_genes))\n",
    "\n",
    "# Create DataFrame\n",
    "sample_ids = [f\"SAMPLE_{i:03d}\" for i in range(n_samples)]\n",
    "gene_ids = [f\"GENE_{i:04d}\" for i in range(n_genes)]\n",
    "X_raw = pd.DataFrame(base_counts, columns=gene_ids, index=sample_ids)\n",
    "\n",
    "# Clinical data\n",
    "clinical = pd.DataFrame({\n",
    "    'sample_id': sample_ids,\n",
    "    'age': np.random.randint(40, 80, n_samples),\n",
    "    'gender': np.random.choice(['Male', 'Female'], n_samples),\n",
    "    'stage': np.random.choice(['I', 'II', 'III', 'IV'], n_samples),\n",
    "    'response': np.random.binomial(1, 0.4, n_samples),\n",
    "    'batch': np.random.choice(['Batch_A', 'Batch_B'], n_samples)\n",
    "})\n",
    "\n",
    "# Add some missing values (realistic scenario)\n",
    "missing_mask = np.random.random(X_raw.shape) < 0.01  # 1% missing\n",
    "X_raw = X_raw.mask(missing_mask)\n",
    "\n",
    "# Save to Drive\n",
    "raw_data_path = f\"{BASE_DIR}/data/raw/gene_expression_raw.csv\"\n",
    "clinical_path = f\"{BASE_DIR}/data/raw/clinical_data.csv\"\n",
    "\n",
    "X_raw.to_csv(raw_data_path)\n",
    "clinical.to_csv(clinical_path, index=False)\n",
    "\n",
    "print(f\"âœ… Sample data created and saved to Drive:\")\n",
    "print(f\"   â€¢ Expression data: {raw_data_path}\")\n",
    "print(f\"   â€¢ Clinical data: {clinical_path}\")\n",
    "print(f\"\\nğŸ“Š Dataset info:\")\n",
    "print(f\"   â€¢ Samples: {X_raw.shape[0]}\")\n",
    "print(f\"   â€¢ Genes: {X_raw.shape[1]}\")\n",
    "print(f\"   â€¢ Missing values: {X_raw.isna().sum().sum()}\")\n",
    "\n",
    "# Display previews\n",
    "print(\"\\nğŸ“‹ Gene expression preview:\")\n",
    "display(X_raw.iloc[:5, :5])\n",
    "\n",
    "print(\"\\nğŸ“‹ Clinical data preview:\")\n",
    "display(clinical.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ How to Use Your Own Data\n",
    "\n",
    "**Option 1: Upload via Colab Interface**\n",
    "```python\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Click to upload\n",
    "# Then move to Drive:\n",
    "!mv your_file.csv /content/drive/MyDrive/OmicSelector2/data/raw/\n",
    "```\n",
    "\n",
    "**Option 2: Upload Directly to Drive**\n",
    "1. Open Google Drive in browser\n",
    "2. Navigate to `OmicSelector2/data/raw/`\n",
    "3. Drag and drop your files\n",
    "4. Refresh this notebook and load the files\n",
    "\n",
    "**Option 3: Share from Another Drive**\n",
    "```python\n",
    "# If data is shared with you, mount and access\n",
    "shared_data_path = '/content/drive/Shareddrives/YourTeam/data.csv'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Drive (replace with your file paths)\n",
    "print(\"ğŸ“‚ Loading data from Google Drive...\\n\")\n",
    "\n",
    "X_raw = pd.read_csv(raw_data_path, index_col=0)\n",
    "clinical = pd.read_csv(clinical_path)\n",
    "y = clinical['response'].values\n",
    "\n",
    "print(f\"âœ… Data loaded successfully!\")\n",
    "print(f\"   â€¢ Expression matrix: {X_raw.shape}\")\n",
    "print(f\"   â€¢ Clinical data: {clinical.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Step 2: Quality Control Analysis\n",
    "\n",
    "Assess data quality before preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” Quality Control Analysis\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Define threshold for missing values (as percentage)\n",
    "MISSING_THRESHOLD_PCT = 5  # Keep genes/samples with â‰¤5% missing values\n",
    "\n",
    "# 1. Missing values\n",
    "missing_per_gene = X_raw.isna().sum() / len(X_raw) * 100\n",
    "missing_per_sample = X_raw.isna().sum(axis=1) / X_raw.shape[1] * 100\n",
    "\n",
    "print(f\"ğŸ“Š Missing Values:\")\n",
    "print(f\"   â€¢ Total missing: {X_raw.isna().sum().sum():,}\")\n",
    "print(f\"   â€¢ Genes with >5% missing: {(missing_per_gene > MISSING_THRESHOLD_PCT).sum()}\")\n",
    "print(f\"   â€¢ Samples with >5% missing: {(missing_per_sample > MISSING_THRESHOLD_PCT).sum()}\")\n",
    "\n",
    "# 2. Zero/low expression genes\n",
    "zero_counts = (X_raw == 0).sum(axis=0)\n",
    "low_expr_genes = (zero_counts > len(X_raw) * 0.9).sum()\n",
    "\n",
    "print(f\"\\nğŸ“Š Expression Levels:\")\n",
    "print(f\"   â€¢ Genes with >90% zeros: {low_expr_genes}\")\n",
    "\n",
    "# 3. Sample-level QC\n",
    "lib_sizes = X_raw.sum(axis=1)\n",
    "print(f\"\\nğŸ“Š Library Sizes (total counts per sample):\")\n",
    "print(f\"   â€¢ Mean: {lib_sizes.mean():,.0f}\")\n",
    "print(f\"   â€¢ Median: {lib_sizes.median():,.0f}\")\n",
    "print(f\"   â€¢ Min: {lib_sizes.min():,.0f}\")\n",
    "print(f\"   â€¢ Max: {lib_sizes.max():,.0f}\")\n",
    "\n",
    "# 4. Variance analysis\n",
    "gene_variances = X_raw.var(axis=0)\n",
    "low_var_genes = (gene_variances < gene_variances.quantile(0.1)).sum()\n",
    "\n",
    "print(f\"\\nğŸ“Š Gene Variance:\")\n",
    "print(f\"   â€¢ Genes in bottom 10% variance: {low_var_genes}\")\n",
    "\n",
    "# Visualize QC metrics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Missing values heatmap (first 50 genes)\n",
    "sns.heatmap(X_raw.iloc[:, :50].isna(), cbar=False, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Missing Values Pattern (First 50 Genes)')\n",
    "axes[0, 0].set_xlabel('Genes')\n",
    "axes[0, 0].set_ylabel('Samples')\n",
    "\n",
    "# Library size distribution\n",
    "axes[0, 1].hist(lib_sizes, bins=30, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Library Size (Total Counts)')\n",
    "axes[0, 1].set_ylabel('Number of Samples')\n",
    "axes[0, 1].set_title('Library Size Distribution')\n",
    "axes[0, 1].axvline(lib_sizes.median(), color='red', linestyle='--', label='Median')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Zero count distribution\n",
    "axes[0, 2].hist(zero_counts, bins=50, edgecolor='black')\n",
    "axes[0, 2].set_xlabel('Number of Zeros')\n",
    "axes[0, 2].set_ylabel('Number of Genes')\n",
    "axes[0, 2].set_title('Zero Count Distribution Across Genes')\n",
    "axes[0, 2].axvline(len(X_raw) * 0.9, color='red', linestyle='--', label='90% threshold')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Gene variance distribution\n",
    "axes[1, 0].hist(np.log10(gene_variances + 1), bins=50, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Log10(Variance + 1)')\n",
    "axes[1, 0].set_ylabel('Number of Genes')\n",
    "axes[1, 0].set_title('Gene Variance Distribution')\n",
    "\n",
    "# Sample correlation matrix (sample 20 samples)\n",
    "sample_indices = np.random.choice(len(X_raw), min(20, len(X_raw)), replace=False)\n",
    "corr_matrix = X_raw.iloc[sample_indices].T.corr()\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', center=0, ax=axes[1, 1], cbar_kws={'label': 'Correlation'})\n",
    "axes[1, 1].set_title('Sample-Sample Correlation (20 samples)')\n",
    "\n",
    "# Box plot of top varying genes\n",
    "top_var_genes = gene_variances.nlargest(10).index\n",
    "axes[1, 2].boxplot([X_raw[gene].dropna() for gene in top_var_genes], labels=range(1, 11))\n",
    "axes[1, 2].set_xlabel('Top 10 Variable Genes (rank)')\n",
    "axes[1, 2].set_ylabel('Expression Level')\n",
    "axes[1, 2].set_title('Top Variable Genes Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "qc_plot_path = f\"{BASE_DIR}/plots/preprocessing/01_quality_control.png\"\n",
    "plt.savefig(qc_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nğŸ’¾ QC plots saved to: {qc_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Save QC report\n",
    "qc_report = pd.DataFrame({\n",
    "    'metric': [\n",
    "        'Total samples', 'Total genes', 'Total missing values',\n",
    "        'Genes with >5% missing', 'Samples with >5% missing',\n",
    "        'Genes with >90% zeros', 'Low variance genes (bottom 10%)',\n",
    "        'Mean library size', 'Median library size'\n",
    "    ],\n",
    "    'value': [\n",
    "        X_raw.shape[0], X_raw.shape[1], X_raw.isna().sum().sum(),\n",
    "        (missing_per_gene > MISSING_THRESHOLD_PCT).sum(), (missing_per_sample > MISSING_THRESHOLD_PCT).sum(),\n",
    "        low_expr_genes, low_var_genes,\n",
    "        f\"{lib_sizes.mean():.0f}\", f\"{lib_sizes.median():.0f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "qc_report_path = f\"{BASE_DIR}/data/qc_reports/qc_summary.csv\"\n",
    "qc_report.to_csv(qc_report_path, index=False)\n",
    "print(f\"ğŸ’¾ QC report saved to: {qc_report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Step 3: Data Cleaning\n",
    "\n",
    "Remove low-quality samples and genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§¹ Data Cleaning\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "X_clean = X_raw.copy()\n",
    "removed_genes = []\n",
    "removed_samples = []\n",
    "\n",
    "# 1. Remove genes with too many missing values\n",
    "genes_to_keep = missing_per_gene <= MISSING_THRESHOLD_PCT  # Keep genes with â‰¤5% missing\n",
    "removed_genes.extend(X_clean.columns[~genes_to_keep].tolist())\n",
    "X_clean = X_clean.loc[:, genes_to_keep]\n",
    "print(f\"âœ“ Removed {(~genes_to_keep).sum()} genes with >5% missing values\")\n",
    "\n",
    "# 2. Remove genes with low expression (>90% zeros)\n",
    "zero_counts_clean = (X_clean == 0).sum(axis=0)\n",
    "high_zero_genes = zero_counts_clean > len(X_clean) * 0.9\n",
    "removed_genes.extend(X_clean.columns[high_zero_genes].tolist())\n",
    "X_clean = X_clean.loc[:, ~high_zero_genes]\n",
    "print(f\"âœ“ Removed {high_zero_genes.sum()} genes with >90% zeros\")\n",
    "\n",
    "# 3. Remove low variance genes (bottom 10%)\n",
    "var_threshold = X_clean.var(axis=0).quantile(0.1)\n",
    "low_var = X_clean.var(axis=0) < var_threshold\n",
    "removed_genes.extend(X_clean.columns[low_var].tolist())\n",
    "X_clean = X_clean.loc[:, ~low_var]\n",
    "print(f\"âœ“ Removed {low_var.sum()} low-variance genes (bottom 10%)\")\n",
    "\n",
    "# 4. Handle remaining missing values (impute with median)\n",
    "missing_before = X_clean.isna().sum().sum()\n",
    "if missing_before > 0:\n",
    "    X_clean = X_clean.fillna(X_clean.median())\n",
    "    print(f\"âœ“ Imputed {missing_before} remaining missing values with gene medians\")\n",
    "\n",
    "print(f\"\\nğŸ“Š After cleaning:\")\n",
    "print(f\"   â€¢ Samples: {X_clean.shape[0]} (removed {len(removed_samples)})\")\n",
    "print(f\"   â€¢ Genes: {X_clean.shape[1]} (removed {len(set(removed_genes))})\")\n",
    "print(f\"   â€¢ Data completeness: 100%\")\n",
    "\n",
    "# Save cleaned data\n",
    "clean_data_path = f\"{BASE_DIR}/data/processed/gene_expression_cleaned.csv\"\n",
    "X_clean.to_csv(clean_data_path)\n",
    "print(f\"\\nğŸ’¾ Cleaned data saved to: {clean_data_path}\")\n",
    "\n",
    "# Save list of removed genes\n",
    "removed_genes_df = pd.DataFrame({'gene': list(set(removed_genes))})\n",
    "removed_genes_path = f\"{BASE_DIR}/data/qc_reports/removed_genes.csv\"\n",
    "removed_genes_df.to_csv(removed_genes_path, index=False)\n",
    "print(f\"ğŸ’¾ Removed genes list saved to: {removed_genes_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Step 4: Normalization\n",
    "\n",
    "Apply different normalization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“ˆ Data Normalization\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# We'll create multiple normalized versions\n",
    "normalized_data = {}\n",
    "\n",
    "# 1. Log2 transformation (common for RNA-seq)\n",
    "X_log2 = np.log2(X_clean + 1)  # +1 to avoid log(0)\n",
    "normalized_data['log2'] = X_log2\n",
    "print(\"âœ“ Log2 transformation: log2(x + 1)\")\n",
    "\n",
    "# 2. Z-score normalization (standardization)\n",
    "scaler_zscore = StandardScaler()\n",
    "X_zscore = pd.DataFrame(\n",
    "    scaler_zscore.fit_transform(X_log2),\n",
    "    columns=X_log2.columns,\n",
    "    index=X_log2.index\n",
    ")\n",
    "normalized_data['zscore'] = X_zscore\n",
    "print(\"âœ“ Z-score normalization: (x - mean) / std\")\n",
    "\n",
    "# 3. Robust scaling (less sensitive to outliers)\n",
    "scaler_robust = RobustScaler()\n",
    "X_robust = pd.DataFrame(\n",
    "    scaler_robust.fit_transform(X_log2),\n",
    "    columns=X_log2.columns,\n",
    "    index=X_log2.index\n",
    ")\n",
    "normalized_data['robust'] = X_robust\n",
    "print(\"âœ“ Robust scaling: (x - median) / IQR\")\n",
    "\n",
    "# 4. TPM-like normalization (per-sample)\n",
    "# Total counts per sample = 1 million\n",
    "X_tpm = X_clean.div(X_clean.sum(axis=1), axis=0) * 1e6\n",
    "X_tpm_log = np.log2(X_tpm + 1)\n",
    "normalized_data['tpm_log'] = X_tpm_log\n",
    "print(\"âœ“ TPM-like normalization: (count/total) * 1M, then log2\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Created {len(normalized_data)} normalized versions\")\n",
    "\n",
    "# Visualize normalization effects\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Original (cleaned)\n",
    "sample_gene = X_clean.iloc[:, 0]\n",
    "axes[0].hist(sample_gene, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Original (Cleaned)')\n",
    "axes[0].set_xlabel('Expression Level')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Each normalization method\n",
    "for idx, (name, data) in enumerate(normalized_data.items(), 1):\n",
    "    sample_gene_norm = data.iloc[:, 0]\n",
    "    axes[idx].hist(sample_gene_norm, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{name.upper()}')\n",
    "    axes[idx].set_xlabel('Expression Level')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "# Hide last subplot if not used\n",
    "if len(normalized_data) < 5:\n",
    "    axes[5].axis('off')\n",
    "\n",
    "plt.suptitle('Effect of Different Normalization Methods (Example Gene)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "norm_plot_path = f\"{BASE_DIR}/plots/preprocessing/02_normalization_comparison.png\"\n",
    "plt.savefig(norm_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nğŸ’¾ Normalization comparison plot saved to: {norm_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Save all normalized versions\n",
    "for name, data in normalized_data.items():\n",
    "    save_path = f\"{BASE_DIR}/data/processed/gene_expression_{name}.csv\"\n",
    "    data.to_csv(save_path)\n",
    "    print(f\"ğŸ’¾ Saved {name} normalized data to: {save_path}\")\n",
    "\n",
    "# Use log2 + z-score for further analysis (common choice)\n",
    "X_normalized = X_zscore\n",
    "print(f\"\\nâœ… Using Z-score normalized data for downstream analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Step 5: Train/Test/Validation Split\n",
    "\n",
    "Proper data splitting to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âœ‚ï¸ Data Splitting\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# First split: 60% train, 40% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_normalized, y,\n",
    "    test_size=0.4,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Second split: split temp into 20% validation, 20% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š Split sizes:\")\n",
    "print(f\"   â€¢ Training:   {X_train.shape[0]} samples ({X_train.shape[0]/len(X_normalized)*100:.1f}%)\")\n",
    "print(f\"   â€¢ Validation: {X_val.shape[0]} samples ({X_val.shape[0]/len(X_normalized)*100:.1f}%)\")\n",
    "print(f\"   â€¢ Test:       {X_test.shape[0]} samples ({X_test.shape[0]/len(X_normalized)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nğŸ“Š Class distribution:\")\n",
    "print(f\"   â€¢ Training:   {np.bincount(y_train)}\")\n",
    "print(f\"   â€¢ Validation: {np.bincount(y_val)}\")\n",
    "print(f\"   â€¢ Test:       {np.bincount(y_test)}\")\n",
    "\n",
    "# Visualize split\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "splits = ['Training', 'Validation', 'Test']\n",
    "class_0 = [np.sum(y_train == 0), np.sum(y_val == 0), np.sum(y_test == 0)]\n",
    "class_1 = [np.sum(y_train == 1), np.sum(y_val == 1), np.sum(y_test == 1)]\n",
    "\n",
    "x = np.arange(len(splits))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, class_0, width, label='Class 0 (Non-responder)', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, class_1, width, label='Class 1 (Responder)', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax.set_title('Class Distribution Across Data Splits', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(splits)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "split_plot_path = f\"{BASE_DIR}/plots/preprocessing/03_data_splits.png\"\n",
    "plt.savefig(split_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nğŸ’¾ Split visualization saved to: {split_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Save splits to Drive\n",
    "print(\"\\nğŸ’¾ Saving data splits to Google Drive...\")\n",
    "\n",
    "splits_dir = f\"{BASE_DIR}/data/processed/splits\"\n",
    "os.makedirs(splits_dir, exist_ok=True)\n",
    "\n",
    "X_train.to_csv(f\"{splits_dir}/X_train.csv\")\n",
    "X_val.to_csv(f\"{splits_dir}/X_val.csv\")\n",
    "X_test.to_csv(f\"{splits_dir}/X_test.csv\")\n",
    "\n",
    "pd.DataFrame({'sample_id': X_train.index, 'response': y_train}).to_csv(f\"{splits_dir}/y_train.csv\", index=False)\n",
    "pd.DataFrame({'sample_id': X_val.index, 'response': y_val}).to_csv(f\"{splits_dir}/y_val.csv\", index=False)\n",
    "pd.DataFrame({'sample_id': X_test.index, 'response': y_test}).to_csv(f\"{splits_dir}/y_test.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… All splits saved to: {splits_dir}/\")\n",
    "print(\"\\nğŸ’¡ These splits are ready for feature selection and model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš–ï¸ Step 6: Handle Class Imbalance (Optional)\n",
    "\n",
    "If your classes are imbalanced, here are some strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"âš–ï¸ Class Imbalance Analysis\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Check imbalance ratio\n",
    "class_counts = Counter(y_train)\n",
    "majority_class = max(class_counts, key=class_counts.get)\n",
    "minority_class = min(class_counts, key=class_counts.get)\n",
    "imbalance_ratio = class_counts[majority_class] / class_counts[minority_class]\n",
    "\n",
    "print(f\"ğŸ“Š Training set class distribution:\")\n",
    "print(f\"   â€¢ Class 0: {class_counts[0]} samples\")\n",
    "print(f\"   â€¢ Class 1: {class_counts[1]} samples\")\n",
    "print(f\"   â€¢ Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "if imbalance_ratio > 1.5:\n",
    "    print(f\"\\nâš ï¸ Moderate imbalance detected (ratio > 1.5:1)\")\n",
    "    print(\"\\nğŸ’¡ Recommended strategies:\")\n",
    "    print(\"   1. Use class weights in model training\")\n",
    "    print(\"   2. Use stratified sampling (already done)\")\n",
    "    print(\"   3. Use balanced metrics (F1, balanced accuracy)\")\n",
    "    \n",
    "    if imbalance_ratio > 3:\n",
    "        print(\"   4. Consider SMOTE (Synthetic Minority Over-sampling)\")\n",
    "        print(\"   5. Consider undersampling majority class\")\n",
    "    \n",
    "    # Example: Calculate class weights for sklearn\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    \n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    print(f\"\\nâœ… Computed class weights: {class_weight_dict}\")\n",
    "    print(\"   Use these when training: model.fit(X, y, class_weight=class_weight_dict)\")\n",
    "    \n",
    "    # Save class weights\n",
    "    class_weights_df = pd.DataFrame({\n",
    "        'class': list(class_weight_dict.keys()),\n",
    "        'weight': list(class_weight_dict.values())\n",
    "    })\n",
    "    weights_path = f\"{BASE_DIR}/data/processed/class_weights.csv\"\n",
    "    class_weights_df.to_csv(weights_path, index=False)\n",
    "    print(f\"\\nğŸ’¾ Class weights saved to: {weights_path}\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Classes are relatively balanced (ratio < 1.5:1)\")\n",
    "    print(\"   No special handling needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 7: Create Processing Summary\n",
    "\n",
    "Document everything for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Create comprehensive summary\n",
    "summary = f\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    DATA PREPROCESSING SUMMARY - OmicSelector2\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Google Drive Location: {BASE_DIR}\n",
    "\n",
    "ORIGINAL DATA\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Samples:              {X_raw.shape[0]}\n",
    "Genes:                {X_raw.shape[1]}\n",
    "Missing values:       {X_raw.isna().sum().sum():,}\n",
    "\n",
    "DATA CLEANING\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Removed samples:      {len(removed_samples)}\n",
    "Removed genes:        {len(set(removed_genes))}\n",
    "  â€¢ >5% missing:      {(missing_per_gene > MISSING_THRESHOLD_PCT).sum()}\n",
    "  â€¢ >90% zeros:       {low_expr_genes}\n",
    "  â€¢ Low variance:     {low_var.sum()}\n",
    "\n",
    "FINAL CLEANED DATA\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Samples:              {X_clean.shape[0]}\n",
    "Genes:                {X_clean.shape[1]}\n",
    "Missing values:       0\n",
    "Completeness:         100%\n",
    "\n",
    "NORMALIZATION\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Methods applied:      {', '.join(normalized_data.keys())}\n",
    "Selected method:      Z-score (standardization)\n",
    "\n",
    "DATA SPLITS\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Training:             {X_train.shape[0]} samples ({X_train.shape[0]/len(X_normalized)*100:.1f}%)\n",
    "Validation:           {X_val.shape[0]} samples ({X_val.shape[0]/len(X_normalized)*100:.1f}%)\n",
    "Test:                 {X_test.shape[0]} samples ({X_test.shape[0]/len(X_normalized)*100:.1f}%)\n",
    "\n",
    "Class distribution (train):\n",
    "  â€¢ Class 0:          {np.sum(y_train == 0)} samples\n",
    "  â€¢ Class 1:          {np.sum(y_train == 1)} samples\n",
    "  â€¢ Imbalance ratio:  {imbalance_ratio:.2f}:1\n",
    "\n",
    "FILES SAVED TO GOOGLE DRIVE\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Raw data:             {BASE_DIR}/data/raw/\n",
    "Cleaned data:         {BASE_DIR}/data/processed/gene_expression_cleaned.csv\n",
    "Normalized data:      {BASE_DIR}/data/processed/gene_expression_*.csv\n",
    "Data splits:          {BASE_DIR}/data/processed/splits/\n",
    "QC reports:           {BASE_DIR}/data/qc_reports/\n",
    "Plots:                {BASE_DIR}/plots/preprocessing/\n",
    "\n",
    "NEXT STEPS\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ Data is cleaned and ready for analysis\n",
    "âœ“ Multiple normalized versions available\n",
    "âœ“ Proper train/val/test splits created\n",
    "âœ“ All files persisted in Google Drive\n",
    "\n",
    "â†’ Continue with feature selection (01_basic_feature_selection.ipynb)\n",
    "â†’ Or try stability selection (05_stability_selection.ipynb)\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "summary_path = f\"{BASE_DIR}/data/processed/preprocessing_summary.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Summary saved to: {summary_path}\")\n",
    "print(f\"\\nâœ… All preprocessing complete! Your data is ready in Google Drive.\")\n",
    "print(f\"\\nğŸ“‚ Access your files at: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Summary\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "âœ… **Data Loading**: Import data from Google Drive in various formats  \n",
    "âœ… **Quality Control**: Assess data quality with comprehensive metrics  \n",
    "âœ… **Data Cleaning**: Remove low-quality samples and features  \n",
    "âœ… **Normalization**: Apply multiple normalization strategies  \n",
    "âœ… **Data Splitting**: Create proper train/val/test splits  \n",
    "âœ… **Imbalance Handling**: Strategies for imbalanced datasets  \n",
    "âœ… **Drive Integration**: Save all results persistently to Google Drive  \n",
    "\n",
    "### Your Google Drive Structure\n",
    "\n",
    "```\n",
    "MyDrive/OmicSelector2/\n",
    "â”œâ”€â”€ data/\n",
    "â”‚   â”œâ”€â”€ raw/                    # Original data\n",
    "â”‚   â”œâ”€â”€ processed/              # Cleaned & normalized data\n",
    "â”‚   â”‚   â”œâ”€â”€ splits/             # Train/val/test splits\n",
    "â”‚   â”‚   â””â”€â”€ *.csv               # Various normalized versions\n",
    "â”‚   â””â”€â”€ qc_reports/             # Quality control reports\n",
    "â””â”€â”€ plots/\n",
    "    â””â”€â”€ preprocessing/           # QC and normalization plots\n",
    "```\n",
    "\n",
    "### Best Practices Checklist\n",
    "\n",
    "âœ… Always perform QC before analysis  \n",
    "âœ… Document removed samples/features  \n",
    "âœ… Try multiple normalization methods  \n",
    "âœ… Use stratified splitting for classification  \n",
    "âœ… Keep validation and test sets truly independent  \n",
    "âœ… Save intermediate results to Drive  \n",
    "âœ… Create reproducible reports  \n",
    "\n",
    "### ğŸ“š Next Tutorials\n",
    "\n",
    "Now that your data is preprocessed, continue with:\n",
    "\n",
    "1. **[01_basic_feature_selection.ipynb](01_basic_feature_selection.ipynb)** - Select important biomarkers\n",
    "2. **[05_stability_selection.ipynb](05_stability_selection.ipynb)** - Robust feature selection\n",
    "3. **[07_complete_workflow.ipynb](07_complete_workflow.ipynb)** - Full analysis pipeline\n",
    "\n",
    "### ğŸ’¡ Tips\n",
    "\n",
    "- **Share with Team**: Share your `OmicSelector2` Drive folder with collaborators\n",
    "- **Version Control**: Keep dated versions of important files\n",
    "- **Documentation**: Always save processing summaries\n",
    "- **Backups**: Drive automatically backs up, but export critical results\n",
    "\n",
    "---\n",
    "\n",
    "**Your data is now ready for biomarker discovery! ğŸ§¬ğŸ”¬**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
