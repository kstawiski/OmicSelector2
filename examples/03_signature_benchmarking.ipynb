{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OmicSelector2: Signature Benchmarking\n",
    "\n",
    "This notebook demonstrates the core philosophy of OmicSelector: **automated benchmarking** of biomarker signatures.\n",
    "\n",
    "**The OmicSelector Philosophy:**\n",
    "Instead of trusting a single feature selection method, we:\n",
    "1. Generate signatures with multiple methods\n",
    "2. Test each signature with various ML models\n",
    "3. Compare performance objectively\n",
    "4. Select the signature most **resilient to overfitting**\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Create multiple feature signatures\n",
    "- Benchmark signatures across different models\n",
    "- Compare signature performance statistically\n",
    "- Identify the most robust biomarker set\n",
    "\n",
    "**Prerequisites:**\n",
    "```bash\n",
    "pip install omicselector2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Import OmicSelector2 components\n",
    "from omicselector2.features.classical.lasso import LassoSelector\n",
    "from omicselector2.features.classical.elastic_net import ElasticNetSelector\n",
    "from omicselector2.features.classical.random_forest import RandomForestSelector\n",
    "from omicselector2.features.classical.mrmr import MRMRSelector\n",
    "from omicselector2.features.classical.boruta import BorutaSelector\n",
    "from omicselector2.features.ensemble import EnsembleSelector\n",
    "from omicselector2.features.stability import StabilitySelector\n",
    "\n",
    "from omicselector2.models.classical import (\n",
    "    RandomForestClassifier,\n",
    "    XGBoostClassifier,\n",
    "    LogisticRegressionModel\n",
    ")\n",
    "from omicselector2.training.benchmarking import SignatureBenchmark, Benchmarker\n",
    "from omicselector2.training.cross_validation import StratifiedKFoldSplitter\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure visualization\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Dataset with Known Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate dataset with 20 truly informative genes\n",
    "X, y = make_classification(\n",
    "    n_samples=400,\n",
    "    n_features=500,\n",
    "    n_informative=20,\n",
    "    n_redundant=30,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    flip_y=0.02,\n",
    "    class_sep=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "X = pd.DataFrame(X, columns=[f\"GENE_{i:03d}\" for i in range(X.shape[1])])\n",
    "y = pd.Series(y, name=\"response\")\n",
    "\n",
    "# Store ground truth informative genes\n",
    "ground_truth_genes = [f\"GENE_{i:03d}\" for i in range(20)]\n",
    "\n",
    "# Split into train and test (hold-out test set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {X.shape}\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Ground truth informative genes: {len(ground_truth_genes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Multiple Feature Signatures\n",
    "\n",
    "Generate biomarker signatures using different feature selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store signatures\n",
    "signatures = {}\n",
    "\n",
    "# Signature 1: Lasso (top 30 features)\n",
    "lasso = LassoSelector(alpha=0.01, task='classification', n_features_to_select=30, random_state=42)\n",
    "lasso.fit(X_train, y_train)\n",
    "signatures['Lasso_30'] = lasso.selected_features_\n",
    "\n",
    "# Signature 2: Elastic Net (top 30 features)\n",
    "elastic_net = ElasticNetSelector(\n",
    "    alpha=0.01, l1_ratio=0.7, task='classification', \n",
    "    n_features_to_select=30, random_state=42\n",
    ")\n",
    "elastic_net.fit(X_train, y_train)\n",
    "signatures['ElasticNet_30'] = elastic_net.selected_features_\n",
    "\n",
    "# Signature 3: Random Forest (top 30 features)\n",
    "rf = RandomForestSelector(\n",
    "    n_estimators=100, task='classification',\n",
    "    n_features_to_select=30, random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "signatures['RandomForest_30'] = rf.selected_features_\n",
    "\n",
    "# Signature 4: mRMR (top 30 features)\n",
    "mrmr = MRMRSelector(n_features_to_select=30, random_state=42)\n",
    "mrmr.fit(X_train, y_train)\n",
    "signatures['mRMR_30'] = mrmr.selected_features_\n",
    "\n",
    "# Signature 5: Boruta (all relevant features)\n",
    "boruta = BorutaSelector(task='classification', max_iter=50, random_state=42)\n",
    "boruta.fit(X_train, y_train)\n",
    "signatures[f'Boruta_{len(boruta.selected_features_)}'] = boruta.selected_features_\n",
    "\n",
    "# Signature 6: Ensemble (consensus of top 3 methods)\n",
    "ensemble = EnsembleSelector(\n",
    "    selectors=[lasso, elastic_net, rf],\n",
    "    strategy='soft_voting',\n",
    "    n_features_to_select=25\n",
    ")\n",
    "ensemble.fit(X_train, y_train)\n",
    "signatures['Ensemble_25'] = ensemble.selected_features_\n",
    "\n",
    "# Signature 7: Stability selection (robust features)\n",
    "stability = StabilitySelector(\n",
    "    base_selector=RandomForestSelector(n_estimators=50, task='classification'),\n",
    "    n_bootstraps=50,\n",
    "    threshold=0.6,\n",
    "    sample_fraction=0.8,\n",
    "    n_features_to_select=25,\n",
    "    random_state=42\n",
    ")\n",
    "stability.fit(X_train, y_train)\n",
    "signatures['Stability_25'] = stability.selected_features_\n",
    "\n",
    "# Display signatures\n",
    "print(\"\\nGenerated Signatures:\")\n",
    "for name, features in signatures.items():\n",
    "    # Count overlap with ground truth\n",
    "    overlap = len(set(features) & set(ground_truth_genes))\n",
    "    print(f\"  {name}: {len(features)} features ({overlap}/{len(ground_truth_genes)} ground truth)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmark Each Signature\n",
    "\n",
    "Test each signature with multiple ML models using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'LogisticRegression': LogisticRegressionModel(random_state=42)\n",
    "}\n",
    "\n",
    "# Create benchmarker\n",
    "benchmarker = Benchmarker(\n",
    "    models=models,\n",
    "    cv_strategy=StratifiedKFoldSplitter(n_splits=5, shuffle=True, random_state=42),\n",
    "    metrics=['accuracy', 'auc', 'f1', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "# Benchmark all signatures\n",
    "print(\"Benchmarking signatures (this may take a few minutes)...\\n\")\n",
    "\n",
    "benchmark_results = benchmarker.benchmark_signatures(\n",
    "    X_train, y_train, signatures, verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nBenchmarking complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Signature Performance\n",
    "\n",
    "Visualize and compare cross-validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary table\n",
    "summary_df = benchmarker.get_summary_table()\n",
    "\n",
    "print(\"\\nBenchmark Summary (CV Performance):\")\n",
    "print(summary_df.to_string())\n",
    "\n",
    "# Save to CSV\n",
    "summary_df.to_csv('signature_benchmark_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'signature_benchmark_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize AUC comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: AUC by signature and model\n",
    "auc_data = summary_df.pivot(index='signature', columns='model', values='cv_auc')\n",
    "auc_data.plot(kind='bar', ax=axes[0], width=0.8)\n",
    "axes[0].set_ylabel('AUC-ROC')\n",
    "axes[0].set_xlabel('Signature')\n",
    "axes[0].set_title('Cross-Validation AUC by Signature and Model')\n",
    "axes[0].legend(title='Model')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0.5, 1.0])\n",
    "\n",
    "# Plot 2: Mean AUC per signature (across all models)\n",
    "mean_auc_per_sig = summary_df.groupby('signature')['cv_auc'].mean().sort_values(ascending=False)\n",
    "mean_auc_per_sig.plot(kind='barh', ax=axes[1], color='steelblue')\n",
    "axes[1].set_xlabel('Mean AUC (across all models)')\n",
    "axes[1].set_ylabel('Signature')\n",
    "axes[1].set_title('Average Performance Across Models')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim([0.5, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBest signatures (by mean AUC):\")\n",
    "for sig, auc in mean_auc_per_sig.head(3).items():\n",
    "    print(f\"  {sig}: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Comparison\n",
    "\n",
    "Perform statistical tests to determine if performance differences are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare top 2 signatures statistically\n",
    "top_2_signatures = mean_auc_per_sig.head(2).index.tolist()\n",
    "sig1_name, sig2_name = top_2_signatures\n",
    "\n",
    "print(f\"\\nStatistical Comparison: {sig1_name} vs {sig2_name}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get detailed results for both signatures\n",
    "sig1_results = benchmarker.results[sig1_name]\n",
    "sig2_results = benchmarker.results[sig2_name]\n",
    "\n",
    "# Compare across all models\n",
    "for model_name in models.keys():\n",
    "    sig1_aucs = sig1_results['fold_metrics'][model_name]['auc']\n",
    "    sig2_aucs = sig2_results['fold_metrics'][model_name]['auc']\n",
    "    \n",
    "    # Paired t-test (same folds)\n",
    "    t_stat, p_value = stats.ttest_rel(sig1_aucs, sig2_aucs)\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  {sig1_name}: {np.mean(sig1_aucs):.4f} ± {np.std(sig1_aucs):.4f}\")\n",
    "    print(f\"  {sig2_name}: {np.mean(sig2_aucs):.4f} ± {np.std(sig2_aucs):.4f}\")\n",
    "    print(f\"  Paired t-test: t={t_stat:.3f}, p={p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        winner = sig1_name if np.mean(sig1_aucs) > np.mean(sig2_aucs) else sig2_name\n",
    "        print(f\"  → {winner} is significantly better (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"  → No significant difference (p >= 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Set Evaluation\n",
    "\n",
    "Evaluate the best signature on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best signature\n",
    "best_signature_name = mean_auc_per_sig.idxmax()\n",
    "best_signature = signatures[best_signature_name]\n",
    "\n",
    "print(f\"Best Signature: {best_signature_name}\")\n",
    "print(f\"Number of features: {len(best_signature)}\")\n",
    "print(f\"\\nFeatures:\")\n",
    "for i, gene in enumerate(best_signature[:20], 1):\n",
    "    is_ground_truth = '✓' if gene in ground_truth_genes else ''\n",
    "    print(f\"  {i}. {gene} {is_ground_truth}\")\n",
    "\n",
    "if len(best_signature) > 20:\n",
    "    print(f\"  ... and {len(best_signature) - 20} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on best signature\n",
    "X_train_best = X_train[best_signature]\n",
    "X_test_best = X_test[best_signature]\n",
    "\n",
    "# Try all three models\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "print(\"\\nTest Set Performance (Hold-out Validation):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Train on full training set\n",
    "    model.fit(X_train_best, y_train)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test_best)\n",
    "    y_pred_proba = model.predict_proba(X_test_best)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    test_results[model_name] = {\n",
    "        'auc': test_auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Test AUC: {test_auc:.4f}\")\n",
    "    \n",
    "# Find best model on test set\n",
    "best_test_model = max(test_results, key=lambda k: test_results[k]['auc'])\n",
    "print(f\"\\n→ Best model on test set: {best_test_model} (AUC = {test_results[best_test_model]['auc']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name, results in test_results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, results['y_pred_proba'])\n",
    "    plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {results['auc']:.3f})\", linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title(f'ROC Curves - {best_signature_name} (Test Set)', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Signature Quality Analysis\n",
    "\n",
    "Analyze the quality of the best signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overlap with ground truth\n",
    "overlap_genes = set(best_signature) & set(ground_truth_genes)\n",
    "false_positives = set(best_signature) - set(ground_truth_genes)\n",
    "\n",
    "print(f\"Signature Quality Analysis: {best_signature_name}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total features: {len(best_signature)}\")\n",
    "print(f\"True positives (overlap with ground truth): {len(overlap_genes)}/{len(ground_truth_genes)} ({len(overlap_genes)/len(ground_truth_genes)*100:.1f}%)\")\n",
    "print(f\"False positives (noise features): {len(false_positives)}\")\n",
    "print(f\"Precision: {len(overlap_genes)/len(best_signature)*100:.1f}%\")\n",
    "print(f\"\\nTrue positive genes:\")\n",
    "for gene in sorted(overlap_genes):\n",
    "    print(f\"  ✓ {gene}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Recommendations\n",
    "\n",
    "### The OmicSelector Workflow:\n",
    "\n",
    "1. **Generate Multiple Signatures** - Use diverse feature selection methods\n",
    "2. **Benchmark Systematically** - Test with multiple ML models and CV\n",
    "3. **Compare Objectively** - Use statistics to identify truly better signatures\n",
    "4. **Validate Rigorously** - Hold-out test set evaluation\n",
    "5. **Select Most Robust** - Choose signature resilient to overfitting\n",
    "\n",
    "### Key Findings from This Analysis:\n",
    "\n",
    "- **Best Signature:** The signature that performed best across all models\n",
    "- **Stability Matters:** Signatures selected by ensemble/stability methods often generalize better\n",
    "- **Model Agnostic:** Good signatures perform well across different ML algorithms\n",
    "- **Ground Truth Recovery:** We successfully recovered a substantial portion of informative features\n",
    "\n",
    "### Recommendations for Real Biomarker Discovery:\n",
    "\n",
    "1. Always use **hold-out validation** (or independent cohort validation)\n",
    "2. Test signatures with **multiple models** - performance should be consistent\n",
    "3. Prefer **ensemble/stability methods** for robust feature selection\n",
    "4. Use **statistical tests** to confirm differences are not due to chance\n",
    "5. Consider **biological validation** (pathway enrichment, literature review)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Apply this workflow to your real biomarker data\n",
    "- Experiment with different signature sizes (10, 20, 50, 100 features)\n",
    "- Try additional feature selection methods\n",
    "- Validate top signatures in independent cohorts\n",
    "- Perform pathway enrichment analysis on selected genes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
