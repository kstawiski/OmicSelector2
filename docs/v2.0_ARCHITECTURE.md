# OmicSelector2 v2.0 Architecture Plan

**Version**: 2.0.0
**Date**: November 10, 2025
**Status**: Planning → Implementation

---

## 1. Executive Summary

This document outlines the architecture for OmicSelector2 v2.0, transitioning from a standalone Python library (v1.0) to a full-stack web application with:

- **Backend**: FastAPI REST API + Celery workers
- **Database**: PostgreSQL 15+ with SQLAlchemy ORM
- **Job Queue**: Celery with Redis broker
- **Storage**: MinIO (S3-compatible) for large files
- **Frontend**: React with Material-UI (future phase)
- **Deployment**: Docker Compose for development, Kubernetes for production

**v2.0 Goals**:
- Enable web-based biomarker discovery workflows
- Support asynchronous job processing for long-running analyses
- Provide real-time progress updates via WebSockets
- Scale to multiple concurrent users
- Maintain v1.0 feature selection and training capabilities

---

## 2. System Architecture Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                    Frontend (React + MUI)                        │
│                     [Future Implementation]                      │
└────────────────────────┬────────────────────────────────────────┘
                         │ REST API (JSON)
                         │ WebSocket (real-time updates)
┌────────────────────────▼────────────────────────────────────────┐
│                   API Layer (FastAPI)                            │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐          │
│  │ Auth Router  │  │ Data Router  │  │ Jobs Router  │          │
│  │ /api/v1/auth │  │ /api/v1/data │  │ /api/v1/jobs │          │
│  └──────────────┘  └──────────────┘  └──────────────┘          │
│  ┌──────────────┐  ┌──────────────┐                            │
│  │Results Router│  │Models Router │                            │
│  │/api/v1/results│ │/api/v1/models│                            │
│  └──────────────┘  └──────────────┘                            │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │           Middleware (CORS, Auth, Logging)                │  │
│  └──────────────────────────────────────────────────────────┘  │
└────────────────────────┬───────────────────────────────────────┘
                         │
┌────────────────────────▼───────────────────────────────────────┐
│              Job Queue (Celery + Redis Broker)                  │
│  ┌─────────────────┐  ┌─────────────────┐                      │
│  │ high_priority   │  │ default         │                      │
│  │ (interactive)   │  │ (batch jobs)    │                      │
│  └─────────────────┘  └─────────────────┘                      │
└──────┬─────────────────────────┬─────────────────────────────┘
       │                         │
┌──────▼──────────┐      ┌───────▼─────────────────┐
│  CPU Workers    │      │   GPU Workers           │
│  (Classical ML, │      │   (Deep Learning:       │
│  Preprocessing) │      │   Future - GNNs, VAEs)  │
└──────┬──────────┘      └───────┬─────────────────┘
       │                         │
┌──────▼─────────────────────────▼─────────────────────────────┐
│                Storage & Persistence Layer                    │
│  ┌──────────────────┐  ┌────────────────┐  ┌──────────────┐ │
│  │  PostgreSQL 15+  │  │ MinIO (S3)     │  │ Redis Cache  │ │
│  │  - Users         │  │ - Datasets     │  │ - Job status │ │
│  │  - Jobs          │  │ - Models       │  │ - Results    │ │
│  │  - Results       │  │ - Large files  │  │              │ │
│  └──────────────────┘  └────────────────┘  └──────────────┘ │
└──────────────────────────────────────────────────────────────┘
```

---

## 3. Database Schema Design

### PostgreSQL Tables (SQLAlchemy Models)

#### 3.1 Users Table
```python
class User(Base):
    __tablename__ = "users"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    email = Column(String(255), unique=True, nullable=False, index=True)
    username = Column(String(100), unique=True, nullable=False, index=True)
    hashed_password = Column(String(255), nullable=False)
    full_name = Column(String(255))
    role = Column(Enum(UserRole), default=UserRole.USER)
    is_active = Column(Boolean, default=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

    # Relationships
    datasets = relationship("Dataset", back_populates="owner")
    jobs = relationship("Job", back_populates="user")
```

#### 3.2 Datasets Table
```python
class Dataset(Base):
    __tablename__ = "datasets"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name = Column(String(255), nullable=False)
    description = Column(Text)
    data_type = Column(Enum(DataType), nullable=False)  # bulk_rna_seq, single_cell, etc.
    file_path = Column(String(500))  # S3/MinIO path
    n_samples = Column(Integer)
    n_features = Column(Integer)
    metadata = Column(JSONB)  # Flexible metadata storage
    owner_id = Column(UUID(as_uuid=True), ForeignKey("users.id"))
    created_at = Column(DateTime(timezone=True), server_default=func.now())

    # Relationships
    owner = relationship("User", back_populates="datasets")
    jobs = relationship("Job", back_populates="dataset")
```

#### 3.3 Jobs Table
```python
class Job(Base):
    __tablename__ = "jobs"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    job_type = Column(Enum(JobType), nullable=False)  # feature_selection, training, etc.
    status = Column(Enum(JobStatus), default=JobStatus.PENDING)
    celery_task_id = Column(String(255), unique=True, index=True)

    # Configuration
    config = Column(JSONB, nullable=False)  # Job parameters

    # References
    user_id = Column(UUID(as_uuid=True), ForeignKey("users.id"))
    dataset_id = Column(UUID(as_uuid=True), ForeignKey("datasets.id"))

    # Execution details
    started_at = Column(DateTime(timezone=True))
    completed_at = Column(DateTime(timezone=True))
    error_message = Column(Text)

    # Results
    result_id = Column(UUID(as_uuid=True), ForeignKey("results.id"))

    # Timestamps
    created_at = Column(DateTime(timezone=True), server_default=func.now())

    # Relationships
    user = relationship("User", back_populates="jobs")
    dataset = relationship("Dataset", back_populates="jobs")
    result = relationship("Result", back_populates="job", uselist=False)
```

#### 3.4 Results Table
```python
class Result(Base):
    __tablename__ = "results"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    job_id = Column(UUID(as_uuid=True), ForeignKey("jobs.id"))

    # Result data
    selected_features = Column(ARRAY(String))  # For feature selection
    metrics = Column(JSONB)  # Performance metrics
    artifacts_path = Column(String(500))  # S3/MinIO path to plots, models

    created_at = Column(DateTime(timezone=True), server_default=func.now())

    # Relationships
    job = relationship("Job", back_populates="result")
```

### Enums
```python
class UserRole(str, Enum):
    USER = "user"
    RESEARCHER = "researcher"
    ADMIN = "admin"

class DataType(str, Enum):
    BULK_RNA_SEQ = "bulk_rna_seq"
    SINGLE_CELL_RNA_SEQ = "single_cell_rna_seq"
    METHYLATION = "methylation"
    CNV = "copy_number"
    PROTEOMICS = "proteomics"

class JobType(str, Enum):
    FEATURE_SELECTION = "feature_selection"
    MODEL_TRAINING = "model_training"
    HYPERPARAMETER_TUNING = "hyperparameter_tuning"
    BENCHMARKING = "benchmarking"
    PREPROCESSING = "preprocessing"

class JobStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
```

---

## 4. API Endpoints Specification

### Base URL: `/api/v1`

#### 4.1 Authentication (`/api/v1/auth`)

**POST /auth/register**
```json
Request:
{
  "email": "user@example.com",
  "username": "researcher1",
  "password": "securepassword",
  "full_name": "Jane Researcher"
}

Response (201):
{
  "id": "uuid",
  "email": "user@example.com",
  "username": "researcher1",
  "created_at": "2025-11-10T12:00:00Z"
}
```

**POST /auth/login**
```json
Request:
{
  "username": "researcher1",
  "password": "securepassword"
}

Response (200):
{
  "access_token": "eyJ0eXAiOiJKV1QiLCJhbGc...",
  "token_type": "bearer",
  "expires_in": 3600
}
```

**GET /auth/me** (requires auth)
```json
Response (200):
{
  "id": "uuid",
  "email": "user@example.com",
  "username": "researcher1",
  "role": "user"
}
```

#### 4.2 Data Management (`/api/v1/data`)

**POST /data/upload** (requires auth, multipart/form-data)
```
Request:
file: <binary data>
name: "TCGA_BLCA_expression"
data_type: "bulk_rna_seq"
description: "Bladder cancer RNA-seq"

Response (201):
{
  "dataset_id": "uuid",
  "name": "TCGA_BLCA_expression",
  "n_samples": 412,
  "n_features": 20531,
  "file_path": "s3://datasets/uuid/data.h5ad"
}
```

**GET /data/{dataset_id}** (requires auth)
```json
Response (200):
{
  "id": "uuid",
  "name": "TCGA_BLCA_expression",
  "data_type": "bulk_rna_seq",
  "n_samples": 412,
  "n_features": 20531,
  "created_at": "2025-11-10T12:00:00Z",
  "metadata": {
    "platform": "Illumina HiSeq",
    "normalization": "TPM"
  }
}
```

**GET /data** (requires auth, pagination)
```json
Response (200):
{
  "items": [
    {
      "id": "uuid",
      "name": "TCGA_BLCA_expression",
      "data_type": "bulk_rna_seq",
      "n_samples": 412,
      "created_at": "2025-11-10T12:00:00Z"
    }
  ],
  "total": 1,
  "page": 1,
  "size": 50
}
```

**DELETE /data/{dataset_id}** (requires auth)
```json
Response (204): No content
```

#### 4.3 Job Management (`/api/v1/jobs`)

**POST /jobs/feature-selection** (requires auth)
```json
Request:
{
  "dataset_id": "uuid",
  "methods": ["lasso", "elastic_net", "rf_importance"],
  "config": {
    "cv_folds": 5,
    "n_features_to_select": 50,
    "stability": {
      "enabled": true,
      "n_bootstraps": 100,
      "threshold": 0.7
    }
  }
}

Response (201):
{
  "job_id": "uuid",
  "job_type": "feature_selection",
  "status": "pending",
  "created_at": "2025-11-10T12:00:00Z"
}
```

**GET /jobs/{job_id}** (requires auth)
```json
Response (200):
{
  "job_id": "uuid",
  "job_type": "feature_selection",
  "status": "running",
  "progress": 45,
  "created_at": "2025-11-10T12:00:00Z",
  "started_at": "2025-11-10T12:00:05Z"
}
```

**GET /jobs** (requires auth, pagination)
```json
Response (200):
{
  "items": [
    {
      "job_id": "uuid",
      "job_type": "feature_selection",
      "status": "completed",
      "created_at": "2025-11-10T12:00:00Z"
    }
  ],
  "total": 1,
  "page": 1,
  "size": 50
}
```

**DELETE /jobs/{job_id}** (requires auth, cancel running job)
```json
Response (200):
{
  "job_id": "uuid",
  "status": "cancelled"
}
```

#### 4.4 Results (`/api/v1/results`)

**GET /results/{result_id}** (requires auth)
```json
Response (200):
{
  "result_id": "uuid",
  "job_id": "uuid",
  "selected_features": ["GENE_0001", "GENE_0005", "GENE_0012"],
  "metrics": {
    "stability_scores": {
      "GENE_0001": 0.95,
      "GENE_0005": 0.87
    }
  },
  "artifacts_path": "s3://results/uuid/",
  "created_at": "2025-11-10T12:05:00Z"
}
```

---

## 5. Celery Task Definitions

### Task Queue Structure

**Queues**:
- `high_priority`: Interactive jobs (< 5 min expected)
- `default`: Standard batch jobs (5-60 min)
- `low_priority`: Long-running jobs (> 1 hour)

### Task Definitions

#### 5.1 Feature Selection Task
```python
@celery_app.task(bind=True, name="tasks.feature_selection")
def run_feature_selection(
    self,
    job_id: str,
    dataset_path: str,
    methods: list[str],
    config: dict
) -> dict:
    """Run feature selection job.

    Args:
        self: Celery task instance
        job_id: UUID of job
        dataset_path: S3 path to dataset
        methods: List of feature selection methods
        config: Job configuration

    Returns:
        dict with selected_features, metrics, artifacts_path
    """
    # Update job status to RUNNING
    update_job_status(job_id, JobStatus.RUNNING)

    try:
        # Load data from S3
        data = load_from_s3(dataset_path)

        # Run feature selection
        results = {}
        for i, method in enumerate(methods):
            # Update progress
            self.update_state(
                state='PROGRESS',
                meta={'current': i+1, 'total': len(methods)}
            )

            selector = get_selector(method, config)
            selector.fit(data.X, data.y)
            results[method] = selector.selected_features_

        # Save results to S3
        artifacts_path = save_results_to_s3(job_id, results)

        # Update job status to COMPLETED
        update_job_status(job_id, JobStatus.COMPLETED)

        return {
            "selected_features": results,
            "artifacts_path": artifacts_path
        }

    except Exception as e:
        # Update job status to FAILED
        update_job_status(job_id, JobStatus.FAILED, error_message=str(e))
        raise
```

#### 5.2 Model Training Task
```python
@celery_app.task(bind=True, name="tasks.model_training")
def run_model_training(
    self,
    job_id: str,
    dataset_path: str,
    features: list[str],
    model_config: dict
) -> dict:
    """Run model training job."""
    # Implementation similar to feature_selection
    pass
```

---

## 6. Docker Compose Configuration

### Development Environment

```yaml
version: '3.8'

services:
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: omicselector2
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: ${DB_PASSWORD:-devpassword}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data

  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    command: uvicorn omicselector2.api.main:app --host 0.0.0.0 --port 8000 --reload
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql://admin:${DB_PASSWORD:-devpassword}@postgres:5432/omicselector2
      REDIS_URL: redis://redis:6379
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
      SECRET_KEY: ${SECRET_KEY:-dev-secret-key-change-in-production}
    ports:
      - "8000:8000"
    volumes:
      - ./src:/app/src

  worker:
    build:
      context: .
      dockerfile: docker/Dockerfile.worker
    command: celery -A omicselector2.tasks worker --loglevel=info --concurrency=2
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql://admin:${DB_PASSWORD:-devpassword}@postgres:5432/omicselector2
      REDIS_URL: redis://redis:6379
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
    volumes:
      - ./src:/app/src

  flower:
    image: mher/flower
    command: celery --broker=redis://redis:6379 flower --port=5555
    depends_on:
      - redis
    ports:
      - "5555:5555"
    environment:
      CELERY_BROKER_URL: redis://redis:6379

volumes:
  postgres_data:
  minio_data:
```

---

## 7. Implementation Phases

### Phase 7.1: Core Infrastructure (Week 1-2)
- [x] Create v2.0 architecture document
- [ ] Set up project structure (api/, tasks/, models/)
- [ ] Initialize FastAPI application
- [ ] Configure SQLAlchemy with Alembic migrations
- [ ] Set up Celery with Redis
- [ ] Create Docker Compose configuration
- [ ] Write infrastructure integration tests

### Phase 7.2: Authentication & User Management (Week 2-3)
- [ ] Implement User model
- [ ] Create JWT authentication
- [ ] Build auth endpoints (register, login, me)
- [ ] Add role-based access control
- [ ] Write authentication tests

### Phase 7.3: Data Management (Week 3-4)
- [ ] Implement Dataset model
- [ ] Create S3/MinIO client
- [ ] Build data upload endpoint
- [ ] Implement data validation
- [ ] Add dataset CRUD endpoints
- [ ] Write data management tests

### Phase 7.4: Job System (Week 4-6)
- [ ] Implement Job and Result models
- [ ] Create feature selection Celery task
- [ ] Build job submission endpoint
- [ ] Implement job status tracking
- [ ] Add WebSocket for real-time updates
- [ ] Write job system tests

### Phase 7.5: Integration & Testing (Week 6-7)
- [ ] End-to-end workflow tests
- [ ] Performance testing
- [ ] Security audit
- [ ] Documentation
- [ ] Deployment guide

---

## 8. Testing Strategy

### Integration Tests
```python
# tests/integration/test_api/test_feature_selection_workflow.py

async def test_complete_feature_selection_workflow(
    client: TestClient,
    auth_headers: dict,
    sample_dataset_file: Path
):
    """Test complete workflow: upload data → run job → get results."""

    # 1. Upload dataset
    with open(sample_dataset_file, "rb") as f:
        response = await client.post(
            "/api/v1/data/upload",
            files={"file": f},
            data={"name": "test_data", "data_type": "bulk_rna_seq"},
            headers=auth_headers
        )
    assert response.status_code == 201
    dataset_id = response.json()["dataset_id"]

    # 2. Submit feature selection job
    response = await client.post(
        "/api/v1/jobs/feature-selection",
        json={
            "dataset_id": dataset_id,
            "methods": ["lasso", "rf_importance"],
            "config": {"cv_folds": 3, "n_features_to_select": 20}
        },
        headers=auth_headers
    )
    assert response.status_code == 201
    job_id = response.json()["job_id"]

    # 3. Wait for job completion
    max_wait = 60  # seconds
    for _ in range(max_wait):
        response = await client.get(
            f"/api/v1/jobs/{job_id}",
            headers=auth_headers
        )
        status = response.json()["status"]
        if status == "completed":
            break
        await asyncio.sleep(1)

    assert status == "completed"

    # 4. Get results
    result_id = response.json()["result_id"]
    response = await client.get(
        f"/api/v1/results/{result_id}",
        headers=auth_headers
    )
    assert response.status_code == 200
    assert "selected_features" in response.json()
```

---

## 9. Security Considerations

### Authentication
- JWT tokens with 1-hour expiration
- Secure password hashing (bcrypt)
- Token refresh mechanism
- Session management

### Authorization
- Role-based access control (USER, RESEARCHER, ADMIN)
- Resource ownership validation
- API rate limiting

### Data Protection
- Encryption at rest (PostgreSQL, S3)
- Encryption in transit (TLS/SSL)
- Input validation (Pydantic models)
- SQL injection prevention (SQLAlchemy ORM)

### Audit Logging
- All data access logged
- User actions tracked
- Compliance with HIPAA/GDPR

---

## 10. Next Steps

1. **Initialize FastAPI application structure**
2. **Set up database with SQLAlchemy and Alembic**
3. **Configure Celery workers**
4. **Create Docker Compose environment**
5. **Implement authentication endpoints**
6. **Build data upload functionality**
7. **Create feature selection job endpoint**
8. **Write comprehensive integration tests**

---

**Document Status**: Complete - Ready for Implementation
**Last Updated**: November 10, 2025
